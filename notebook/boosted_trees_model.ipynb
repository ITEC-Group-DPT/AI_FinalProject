{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library\n",
    "We are using the Turi Create library for implementing the Boosted Trees Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import turicreate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "We will be using the same given [LendingClub](https://www.lendingclub.com/) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans = turicreate.SFrame('../data/lending-club-data.sframe/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Column Definition\n",
    "\n",
    "The target column (label column) of the dataset that we are interested in is called `bad_loans`. In this column **1** means a risky (bad) loan **0** means a safe  loan.\n",
    "\n",
    "We reassign the target to be:\n",
    "* **+1** as a safe  loan, \n",
    "* **-1** as a risky (bad) loan. \n",
    "\n",
    "We put this in a new column called `safe_loans` and define it as `target` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans['safe_loans'] = loans['bad_loans'].apply(lambda x : +1 if x==0 else -1)\n",
    "loans = loans.remove_column('bad_loans')\n",
    "target = 'safe_loans' # prediction target (y) (+1 means safe, -1 is risky)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Selection\n",
    "Like previous assignment, we will be using a subset of features (categorical and numeric). The features we will be using are **described in the code comments** below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['grade',                     # grade of the loan\n",
    "            'sub_grade',                 # sub-grade of the loan\n",
    "            'short_emp',                 # one year or less of employment\n",
    "            'emp_length_num',            # number of years of employment\n",
    "            'home_ownership',            # home_ownership status: own, mortgage or rent\n",
    "            'dti',                       # debt to income ratio\n",
    "            'purpose',                   # the purpose of the loan\n",
    "            'term',                      # the term of the loan\n",
    "            'last_delinq_none',          # has borrower had a delinquincy\n",
    "            'last_major_derog_none',     # has borrower had 90 day or worse rating\n",
    "            'revol_util',                # percent of available credit being used\n",
    "            'total_rec_late_fee',        # total late fees received to day\n",
    "           ]\n",
    "\n",
    "# Extract the feature columns and target column\n",
    "loans = loans[features + [target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Balancing\n",
    "One way to combat class imbalance is to undersample the larger class until the class distribution is approximately half and half. Here, we will undersample the larger class (safe loans) in order to balance out our dataset. This means we are throwing away many data points. We used `seed=1` so everyone gets the same results.\n",
    "\n",
    "We do this in order to help the algorithm studies both classes equally so it can perform more precise predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_loans_raw = loans[loans[target] == +1]\n",
    "risky_loans_raw = loans[loans[target] == -1]\n",
    "\n",
    "# Since there are fewer risky loans than safe loans, find the ratio of the sizes\n",
    "# and use that percentage to undersample the safe loans.\n",
    "percentage = len(risky_loans_raw)/float(len(safe_loans_raw))\n",
    "\n",
    "risky_loans = risky_loans_raw\n",
    "safe_loans = safe_loans_raw.sample(percentage, seed = 1)\n",
    "\n",
    "# Append the risky_loans with the downsampled version of safe_loans\n",
    "loans_data = risky_loans.append(safe_loans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "80% of the original data will be randomly split into training set `(train_data)` and 20% will be randomly split into test set `(test_data)`. We used `seed=1` so everyone gets the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = loans_data.random_split(0.8, seed = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosted Trees Model Building\n",
    "By using Turi Create we use its `boosted_trees_classifier` class to create the model. The parameters are:\n",
    "\n",
    "* `train_data`: the input data for the algorithm to train on.\n",
    "\n",
    "* `validation_set`: set to None because we don't have a validation set.\n",
    "\n",
    "* `target`: is the target column which is `safe_loans`.\n",
    "\n",
    "* `features`: are the features the algorithm will use to learn.\n",
    "\n",
    "* `max_iterations`: the number of trees grown for the model **(this will be covered in the algorithm explaination below)**\n",
    "\n",
    "* `min_child_weight`: the min weight of a child node. In the training process, if weight of a child nonde smaller than min_child_weight then that node won't be divided\n",
    "\n",
    "* `column_subsample`: percentage of sample getting from training set to improve decision tree\n",
    "\n",
    "* `random_seed`: this is the seed for randomization when selecting data points as training data for different trees and subset features for each tree\n",
    "\n",
    "* `max_depth`: the maximum depth allowed for all trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Boosted trees classifier:</pre>"
      ],
      "text/plain": [
       "Boosted trees classifier:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of examples          : 37224</pre>"
      ],
      "text/plain": [
       "Number of examples          : 37224"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of classes           : 2</pre>"
      ],
      "text/plain": [
       "Number of classes           : 2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of feature columns   : 12</pre>"
      ],
      "text/plain": [
       "Number of feature columns   : 12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of unpacked features : 12</pre>"
      ],
      "text/plain": [
       "Number of unpacked features : 12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+--------------+-------------------+-------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+--------------+-------------------+-------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| Iteration | Elapsed Time | Training Accuracy | Training Log Loss |</pre>"
      ],
      "text/plain": [
       "| Iteration | Elapsed Time | Training Accuracy | Training Log Loss |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+--------------+-------------------+-------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+--------------+-------------------+-------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 1         | 0.029111     | 0.660783          | 0.654343          |</pre>"
      ],
      "text/plain": [
       "| 1         | 0.029111     | 0.660783          | 0.654343          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 2         | 0.062581     | 0.673920          | 0.630870          |</pre>"
      ],
      "text/plain": [
       "| 2         | 0.062581     | 0.673920          | 0.630870          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 3         | 0.091172     | 0.678084          | 0.616042          |</pre>"
      ],
      "text/plain": [
       "| 3         | 0.091172     | 0.678084          | 0.616042          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 4         | 0.120657     | 0.681952          | 0.604942          |</pre>"
      ],
      "text/plain": [
       "| 4         | 0.120657     | 0.681952          | 0.604942          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 5         | 0.160300     | 0.685203          | 0.596795          |</pre>"
      ],
      "text/plain": [
       "| 5         | 0.160300     | 0.685203          | 0.596795          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 10        | 0.319245     | 0.704116          | 0.566502          |</pre>"
      ],
      "text/plain": [
       "| 10        | 0.319245     | 0.704116          | 0.566502          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 50        | 1.206906     | 0.777402          | 0.488472          |</pre>"
      ],
      "text/plain": [
       "| 50        | 1.206906     | 0.777402          | 0.488472          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 100       | 2.309374     | 0.840479          | 0.420847          |</pre>"
      ],
      "text/plain": [
       "| 100       | 2.309374     | 0.840479          | 0.420847          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+--------------+-------------------+-------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+--------------+-------------------+-------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "boosted_trees_model = turicreate.boosted_trees_classifier.create(train_data,\n",
    "                                                            validation_set = None,\n",
    "                                                            target = target,\n",
    "                                                            features = features,\n",
    "                                                            max_iterations = 100,\n",
    "                                                            min_child_weight = 1,\n",
    "                                                            column_subsample =  0.85, \n",
    "                                                            random_seed = 1,\n",
    "                                                            max_depth = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the summary of the model after building:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class                          : BoostedTreesClassifier\n",
      "\n",
      "Schema\n",
      "------\n",
      "Number of examples             : 37224\n",
      "Number of feature columns      : 12\n",
      "Number of unpacked features    : 12\n",
      "Number of classes              : 2\n",
      "\n",
      "Settings\n",
      "--------\n",
      "Number of trees                : 100\n",
      "Max tree depth                 : 10\n",
      "Training time (sec)            : 2.3097\n",
      "Training accuracy              : 0.8405\n",
      "Training log_loss              : 0.4208\n",
      "Training auc                   : 0.9275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "boosted_trees_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Comparison With The Decision Tree Model\n",
    "By using the same training data set `(train_data)`. We also build a Decision Tree Model with them same `target` and `features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Decision tree classifier:</pre>"
      ],
      "text/plain": [
       "Decision tree classifier:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of examples          : 37224</pre>"
      ],
      "text/plain": [
       "Number of examples          : 37224"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of classes           : 2</pre>"
      ],
      "text/plain": [
       "Number of classes           : 2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of feature columns   : 12</pre>"
      ],
      "text/plain": [
       "Number of feature columns   : 12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of unpacked features : 12</pre>"
      ],
      "text/plain": [
       "Number of unpacked features : 12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+--------------+-------------------+-------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+--------------+-------------------+-------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| Iteration | Elapsed Time | Training Accuracy | Training Log Loss |</pre>"
      ],
      "text/plain": [
       "| Iteration | Elapsed Time | Training Accuracy | Training Log Loss |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+--------------+-------------------+-------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+--------------+-------------------+-------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 1         | 0.035519     | 0.665538          | 0.606828          |</pre>"
      ],
      "text/plain": [
       "| 1         | 0.035519     | 0.665538          | 0.606828          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+--------------+-------------------+-------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+--------------+-------------------+-------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decision_tree_model = turicreate.decision_tree_classifier.create(train_data,\n",
    "                                                            validation_set = None,\n",
    "                                                            target = target,\n",
    "                                                            features = features,\n",
    "                                                            max_depth = 10,\n",
    "                                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, have a quick comparison between two models with their corresponding accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree model's accuracy: 0.6274235243429557\n",
      "Boosted Trees model's accuracy: 0.6140672124084446\n"
     ]
    }
   ],
   "source": [
    "print(\"Decision Tree model's accuracy:\", decision_tree_model.evaluate(test_data)['accuracy'])\n",
    "print(\"Boosted Trees model's accuracy:\", boosted_trees_model.evaluate(test_data)['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that **most of the time**, the accuracy of the **Boosted Trees model** is ***higher*** than the **Decision Tree model**.\n",
    "\n",
    "If your result is opposite, you can try **rebuild** the **Boosted Trees model** by executing its code cell again. Then try the accuracy test again (or just hit **Run All**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Boosted Tree Model Algorithm Explaination**\n",
    "\n",
    "### **Main Idea**\n",
    "\n",
    "Like other boosting methods, gradient boosting combines weak \"learners\" into a single strong learner in an iterative fashion. It is easiest to explain in the least-squares regression setting, where the goal is to \"teach\" a model $F$ to predict values of the form $\\hat{y} = F(x)$ by minimizing the mean squared error $\\frac{1}{n} \\sum \\limits _{i} (\\hat{y_{i}} - y_{i})^{2}$, where $i$ indexes over some training set of size $n$ of actual values of the output variable $y$:\n",
    "- $\\hat{y_{i}}$:  the predicted value $F(x_{i})$\n",
    "- $y_{i}$: the observed value\n",
    "- $n$: the number of samples in $y$\n",
    "\n",
    "Now, let us consider a gradient boosting algorithm with $M$ stages. At each stage $m(1 <= m <= M)$ of gradient boosting, suppose some imperfect mode $F_{m}$ for low $m$, this model may simply return $\\hat{y_{i}} = \\bar{y}$, , where the RHS is the mean of $y$).\n",
    "\n",
    "In order to improve $F_{m}$, our algorithm should add some new estimator, $h_{m}(x)$. Thus, $F_{m+1}(x) = F_{m}(x) + h_{m}(x) = y$ or, equivalently, $h_{m}(x) = y - F_{m}(x)$\n",
    "\n",
    "Therefore, gradient boosting will fit $h$ to the residual $y - F_{m}(x)$. As in other boosting variants, each $F_{m+1}$ attempts to correct the errors of its predecessor $F_{m}$. \n",
    "\n",
    "A generalization of this idea to loss functions other than squared error, and to classification and ranking problems, follows from the observation that residuals $h_{m}(x)$ for a given model are proportional equivalent to the negative gradients of the mean squared error (MSE) loss function (with respect to $F(x)$:\n",
    "\n",
    "$L_{MSE} = \\frac{1}{n}(y - F(x))^{2}$&emsp;&emsp;&emsp;&emsp;$-\\frac{\\partial L_{MSE}}{\\partial F} = \\frac{2}{n}(y - F(x)) = \\frac{2}{n}h_{m}(x)$\n",
    "\n",
    "So, gradient boosting could be specialized to a gradient descent algorithm, and generalizing it entails \"plugging in\" a different loss and its gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Gradient Tree Boosting Algorithm**\n",
    "\n",
    "1.&emsp;Initialize model with a constant value:&emsp;&emsp;$f_{0}(x) = \\textrm{arg min}_{\\gamma} \\sum \\limits _{i=1} ^{N} L(y_{i}, \\gamma)$<br>\n",
    "2.&emsp;For m = 1 to M:<br><br>\n",
    "&emsp;&emsp;(a)&emsp;For $i = 1,2,...,N$ compute: &emsp;&emsp;$r_{im} = - \\displaystyle \\Bigg[\\frac{\\partial L(y_{i}, f(x_{i}))}{\\partial f(x_{i})}\\Bigg]_{f=f_{m−1}}$<br><br>\n",
    "&emsp;&emsp;(b)&emsp;Fit a regression tree to the targets $r_{im}$ giving terminal regions:&emsp;&emsp;$R_{jm}, j = 1, 2, . . . , J_{m}.$<br><br><br>\n",
    "&emsp;&emsp;(c)&emsp;For $j = 1, 2, . . . , J_{m}$ compute: &emsp;&emsp;&emsp;&emsp;$\\gamma_{jm} = \\underset{\\gamma}{\\textrm{arg min}} \\sum \\limits _{x_{i} \\in R_{jm}} L(y_{i}, f_{m−1}(x_{i}) + \\gamma)$<br><br>\n",
    "&emsp;&emsp;(d)&emsp;Update:&emsp;&emsp;$f_{m}(x) = f_{m−1}(x) + \\sum _{j=1} ^{J_{m}} \\gamma_{jm} I(x \\in R_{jm})$<br><br>\n",
    "3. Output $\\hat{f}(x) = f_{M}(x)$\n",
    "\n",
    "### **Pros and Cons**\n",
    "**Pros**\n",
    "- Often provides predictive accuracy that cannot be trumped\n",
    "- Lots of flexibility - can optimize on different loss functions and provides several hyper parameter tuning options that make the function fit very flexible.\n",
    "- No data pre-processing required - often works great with categorical and numerical values as is.\n",
    "- Handles missing data - imputation not required.\n",
    "\n",
    "**Cons**\n",
    "- Gradient Boosting Models will continue improving to minimize all errors. This can overemphasize outliers and cause overfitting.\n",
    "- Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive.\n",
    "- The high flexibility results in many parameters that interact and influence heavily the behavior of the approach (number of iterations, tree depth, regularization parameters, etc.). This requires a large grid search during tuning.\n",
    "- Less interpretative in nature, although this is easily addressed with various tools.\n",
    "\n",
    "**Visualized Reference From**: https://www.youtube.com/watch?v=TyvYZ26alZs"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
