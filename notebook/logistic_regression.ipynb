{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library\n",
    "We are using the Turicreate library for implementing the Logistic Regression Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import turicreate as tc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "We will be using the same given [LendingClub](https://www.lendingclub.com/) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans = tc.SFrame('../data/lending-club-data.sframe/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Column Definition\n",
    "\n",
    "The target column (label column) of the dataset that we are interested in is called `bad_loans`. In this column **1** means a risky (bad) loan **0** means a safe  loan.\n",
    "\n",
    "We reassign the target to be:\n",
    "* **+1** as a safe  loan, \n",
    "* **-1** as a risky (bad) loan. \n",
    "\n",
    "We put this in a new column called `safe_loans` and define it as `target` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans['safe_loans'] = loans['bad_loans'].apply(lambda x : +1 if x==0 else -1)\n",
    "loans = loans.remove_column('bad_loans')\n",
    "\n",
    "target = 'safe_loans' # prediction target (y) (+1 means safe, -1 is risky)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Selection\n",
    "Like previous assignment, we will be using a subset of features (categorical and numeric). The features we will be using are **described in the code comments** below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['grade',                     # grade of the loan\n",
    "            'sub_grade',                 # sub-grade of the loan\n",
    "            'short_emp',                 # one year or less of employment\n",
    "            'emp_length_num',            # number of years of employment\n",
    "            'home_ownership',            # home_ownership status: own, mortgage or rent\n",
    "            'dti',                       # debt to income ratio\n",
    "            'purpose',                   # the purpose of the loan\n",
    "            'term',                      # the term of the loan\n",
    "            'last_delinq_none',          # has borrower had a delinquincy\n",
    "            'last_major_derog_none',     # has borrower had 90 day or worse rating\n",
    "            'revol_util',                # percent of available credit being used\n",
    "            'total_rec_late_fee',        # total late fees received to day\n",
    "           ]\n",
    "     \n",
    "# Extract the feature columns and target column\n",
    "loans = loans[features + [target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Balancing\n",
    "One way to combat class imbalance is to undersample the larger class until the class distribution is approximately half and half. Here, we will undersample the larger class (safe loans) in order to balance out our dataset. This means we are throwing away many data points. We used `seed=1` so everyone gets the same results.\n",
    "\n",
    "We do this in order to help the algorithm studies both classes equally so it can perform more precise predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_loans_raw = loans[loans[target] == +1]\n",
    "risky_loans_raw = loans[loans[target] == -1]\n",
    "\n",
    "# Since there are fewer risky loans than safe loans, find the ratio of the sizes\n",
    "# and use that percentage to undersample the safe loans.\n",
    "percentage = len(risky_loans_raw)/float(len(safe_loans_raw))\n",
    "\n",
    "risky_loans = risky_loans_raw\n",
    "safe_loans = safe_loans_raw.sample(percentage, seed = 1)\n",
    "\n",
    "# Append the risky_loans with the downsampled version of safe_loans\n",
    "loans_data = risky_loans.append(safe_loans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "80% of the original data will be randomly split into training set `(train_data)` and 20% will be randomly split into test set `(test_data)`. We used `seed=1` so everyone gets the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = loans_data.random_split(.8, seed = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model Building\n",
    "By using Turi Create we use its `logistic_regression_classifier` class to create the model. The parameters are:\n",
    "\n",
    "* `dataset`: the dataset input for training.\n",
    "\n",
    "* `target`: name of the column containing the target variable, which is `safe_loans`.\n",
    "\n",
    "* `features`: list of the columns containing features, which is `feature`.\n",
    "\n",
    "* `validation_set`: the dataset for monitoring the modelâ€™s generalization performance, which is `test_data`.\n",
    "\n",
    "* `seed`: seed for random number generation. Set this value to ensure that the same model is created every time. \n",
    "\n",
    "* `max_iterations`: the maximum number of allowed passes through the data\n",
    "\n",
    "* `solver`: name of the solver to be used to solve the regression. The default value is `auto`, automatically chooses the best solver for the data and model parameters. In this case is Newton method. which works best for datasets with plenty of examples and few features (long datasets).\n",
    "\n",
    "\n",
    "The rest parameter will be automatically setup\n",
    "\n",
    "* `l2_penalty`: the larger this weight, the more the model coefficients thrink toward 0. The default value is 0.01.\n",
    "\n",
    "* `l1_penalty`: the larger this weight, the more the model coefficients thrink toward 0. The default weight is 0 prevents any features from being discarded.\n",
    "\n",
    "* `lbfgs_memory_level`: because the Newton method is applied, we don't need this parameter.\n",
    "\n",
    "* `feature_rescaling`: Feature rescaling is an important pre-processing step that ensures that all features are on the same scale. Because our features are categorical type (string) they will be rescaled to the dummy variables that are used to represent them. The coefficients are returned in original scale of the problem. This process is particularly useful when features vary widely in their ranges.\n",
    "\n",
    "* `convergence_threshhold`: Convergence is tested using variation in the training objective. The variation in the training objective is calculated using the difference between the objective values between two steps. Consider reducing this below the default value (0.01) for a more accurately trained model. Beware of overfitting (i.e a model that works well only on the training data) if this parameter is set to a very low value.\n",
    "\n",
    "* `step_size`: the default is set to 1.0, this is an aggressive setting, reducing this parameter may speed up model training.\n",
    "\n",
    "* `class_weight`: weights the examples in the training data according to the given class weights. If set to None, all classes are supposed to have weight one. The auto mode set the class weight to be inversely proportional to number of examples in the training data with the given class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Logistic regression:</pre>"
      ],
      "text/plain": [
       "Logistic regression:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of examples          : 37224</pre>"
      ],
      "text/plain": [
       "Number of examples          : 37224"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of classes           : 2</pre>"
      ],
      "text/plain": [
       "Number of classes           : 2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of feature columns   : 12</pre>"
      ],
      "text/plain": [
       "Number of feature columns   : 12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of unpacked features : 12</pre>"
      ],
      "text/plain": [
       "Number of unpacked features : 12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of coefficients      : 63</pre>"
      ],
      "text/plain": [
       "Number of coefficients      : 63"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Starting Newton Method</pre>"
      ],
      "text/plain": [
       "Starting Newton Method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+-------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+-------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| Iteration | Passes   | Elapsed Time | Training Accuracy |</pre>"
      ],
      "text/plain": [
       "| Iteration | Passes   | Elapsed Time | Training Accuracy |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+-------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+-------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 1         | 2        | 1.040418     | 0.642865          |</pre>"
      ],
      "text/plain": [
       "| 1         | 2        | 1.040418     | 0.642865          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 2         | 3        | 1.065809     | 0.645551          |</pre>"
      ],
      "text/plain": [
       "| 2         | 3        | 1.065809     | 0.645551          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 3         | 4        | 1.094585     | 0.645954          |</pre>"
      ],
      "text/plain": [
       "| 3         | 4        | 1.094585     | 0.645954          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 4         | 5        | 1.122172     | 0.645954          |</pre>"
      ],
      "text/plain": [
       "| 4         | 5        | 1.122172     | 0.645954          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 5         | 6        | 1.146024     | 0.645954          |</pre>"
      ],
      "text/plain": [
       "| 5         | 6        | 1.146024     | 0.645954          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+-------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+-------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>SUCCESS: Optimal solution found.</pre>"
      ],
      "text/plain": [
       "SUCCESS: Optimal solution found."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre></pre>"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logistic_regression_model = tc.logistic_classifier.create(dataset = train_data,\n",
    "                                                            target = target,\n",
    "                                                            features = features,\n",
    "                                                            validation_set = None,\n",
    "                                                            seed = 1,\n",
    "                                                            solver = 'auto',\n",
    "                                                            max_iterations=100\n",
    "                                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the summary of the model after building:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class                          : LogisticClassifier\n",
      "\n",
      "Schema\n",
      "------\n",
      "Number of coefficients         : 63\n",
      "Number of examples             : 37224\n",
      "Number of classes              : 2\n",
      "Number of feature columns      : 12\n",
      "Number of unpacked features    : 12\n",
      "\n",
      "Hyperparameters\n",
      "---------------\n",
      "L1 penalty                     : 0.0\n",
      "L2 penalty                     : 0.01\n",
      "\n",
      "Training Summary\n",
      "----------------\n",
      "Solver                         : newton\n",
      "Solver iterations              : 5\n",
      "Solver status                  : SUCCESS: Optimal solution found.\n",
      "Training time (sec)            : 1.1528\n",
      "\n",
      "Settings\n",
      "--------\n",
      "Log-likelihood                 : 23377.019\n",
      "\n",
      "Highest Positive Coefficients\n",
      "-----------------------------\n",
      "sub_grade[A1]                  : 0.962\n",
      "sub_grade[A2]                  : 0.7342\n",
      "sub_grade[A3]                  : 0.6628\n",
      "grade[A]                       : 0.5396\n",
      "sub_grade[A4]                  : 0.4526\n",
      "\n",
      "Lowest Negative Coefficients\n",
      "----------------------------\n",
      "purpose[small_business]        : -0.9446\n",
      "sub_grade[G3]                  : -0.7081\n",
      "sub_grade[F4]                  : -0.6018\n",
      "sub_grade[G4]                  : -0.528\n",
      "sub_grade[F5]                  : -0.3735\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Comparison With The Decision Tree Model\n",
    "By using the same training data set `(train_data)`. We also build a Decision Tree Model with them same `target` and `features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Decision tree classifier:</pre>"
      ],
      "text/plain": [
       "Decision tree classifier:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of examples          : 37224</pre>"
      ],
      "text/plain": [
       "Number of examples          : 37224"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of classes           : 2</pre>"
      ],
      "text/plain": [
       "Number of classes           : 2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of feature columns   : 12</pre>"
      ],
      "text/plain": [
       "Number of feature columns   : 12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of unpacked features : 12</pre>"
      ],
      "text/plain": [
       "Number of unpacked features : 12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+--------------+-------------------+-------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+--------------+-------------------+-------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| Iteration | Elapsed Time | Training Accuracy | Training Log Loss |</pre>"
      ],
      "text/plain": [
       "| Iteration | Elapsed Time | Training Accuracy | Training Log Loss |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+--------------+-------------------+-------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+--------------+-------------------+-------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 1         | 0.019609     | 0.640581          | 0.631396          |</pre>"
      ],
      "text/plain": [
       "| 1         | 0.019609     | 0.640581          | 0.631396          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+--------------+-------------------+-------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+--------------+-------------------+-------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decision_tree_model = tc.decision_tree_classifier.create(train_data,\n",
    "                                                            validation_set = None,\n",
    "                                                            target = target,\n",
    "                                                            features = features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, have a quick comparison between two models with their corresponding accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree model's accuracy: 0.6367944851357173\n",
      "Logistic Regression model's accuracy: 0.6427186557518311\n"
     ]
    }
   ],
   "source": [
    "print(\"Decision Tree model's accuracy:\", decision_tree_model.evaluate(test_data)['accuracy'])\n",
    "print(\"Logistic Regression model's accuracy:\", logistic_regression_model.evaluate(test_data)['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that **most of the time** (with `random_seed = None` when building the model), the accuracy of the **Logistic Regression model** is ***higher*** than the **Decision Tree model**.\n",
    "\n",
    "If your result is opposite, you can try **rebuild** the **Logistic Regression model** by executing its code cell again. Then try the accuracy test again (or just hit **Run All**)\n",
    "\n",
    "The result will be 100% different each try (with `random_seed = None` when building the model). So how does this work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Logistic Regression Model Algorithm Explaination**\n",
    "\n",
    "### **Main Idea**\n",
    "\n",
    "Logistic Regression is an algorithm for classification used when the target variable is categorical. The Logistic Regression algorithm comes into the picture when the data has a binary output belonging to one class, i.e., either 0 or 1. \n",
    "\n",
    "Logistic Regression is one of the most simple and commonly used Machine Learning algorithms for two-class classification. It is easy to implement and can be used as the baseline for any binary classification problem. Its basic fundamental concepts are also constructive in deep learning. Logistic regression describes and estimates the relationship between one dependent binary variable and independent variables.\n",
    "\n",
    "Logistic Regression is similar to Linear Regression, except it only predicts something true or false. Also insteal fitting a line to the data visualization, Logistic Regression fits an s-shaped logistic funtion.\n",
    "\n",
    "**Linear Regression Equation**\n",
    "\n",
    "$y = \\beta{0} + \\beta{1}X1 + \\beta{2}X2 + ... + \\beta{n}Xn$<br>\n",
    "\n",
    "**Logistic function**\n",
    "\n",
    "Logistic function is a type of sigmoid function that squishes values between 0 and 1<br>\n",
    "$logistic(z) = {\\frac{1}{1 + e^{-z}}}$\n",
    "\n",
    "\n",
    "### **The Learning Process**\n",
    "**Input**: the training data set `train_data`\n",
    "\n",
    "**Learning**\n",
    "\n",
    "1. Calcualate Linear Regression Equation\n",
    "\n",
    "2. Calculate Logistic Sigmoid Function\n",
    "\n",
    "3. Apply Sigmoid function on Linear Regression:\n",
    "\n",
    "$p = 1/1 + e^{\\beta{0} + \\beta{1}X1 + \\beta{2}X2 + ... + \\beta{n}Xn}$\n",
    "\n",
    "\n",
    "### **Pros and Cons**\n",
    "**Pros**\n",
    " \n",
    "- Donâ€™t need to pick learning rate\n",
    "- Often run faster (not always the case)\n",
    "- Can numerically approximate gradient for you (doesnâ€™t always work out well)\n",
    "\n",
    "\n",
    "**Cons**\n",
    "- More complex\n",
    "- More of a black box unless you learn the specifics\n",
    "\n",
    "\n",
    "\n",
    "**Visualized Reference From**\n",
    "https://www.youtube.com/watch?v=yIYKR4sgzI8\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
